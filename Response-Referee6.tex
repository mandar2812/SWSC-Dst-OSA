\documentclass{article}

\usepackage{graphicx}
\usepackage{txfonts}
\usepackage{subfigure}
\usepackage{epstopdf}
\usepackage{lineno}
\usepackage[authoryear,round]{natbib}
\usepackage[backref]{hyperref}
\usepackage{url}
\usepackage[inline]{enumitem}
\usepackage{verbatim}
\usepackage{csquotes}

\usepackage[authoryear,round]{natbib}
\usepackage[backref]{hyperref}
\usepackage{url}
%%    This version assumes using bibtex with the swsc bibliography style file
\bibliographystyle{swsc.bst}

\begin{document}
\title{Manuscript Ref: swsc160029}
\maketitle
\section*{Revisions}
\begin{enumerate}
\item \emph{Was the size of the training set of N=250 hourly values of Dst for the GP-AR model (and, additionally, V, Bz, VBz for the GP-ARX model) limited by the numerical effort to perform the matrix inversion? If yes, what resources did the inversion require?}

\textbf{Remarks}: 

This is answered in lines $161-165$ \blockquote{The practical implementation of \emph{Gaussian Process} models requires the inversion of the training data kernel matrix $[\mathbf{K} + \sigma^{2} \mathbf{I}]^{-1}$ to calculate the parameters of the predictive distribution $\mathbf{f_*}|\mathbf{X},\mathbf{y},\mathbf{X_*}$. The computational complexity of this inference is dominated by the linear problem $\mathbf{K}^T_{*} [\mathbf{K} + \sigma^{2} \mathbf{I}]^{-1} \mathbf{y}$, which can be solved via Cholesky decomposition, with a time complexity of $O(N^3)$, where $N$ is the number of data points.} 

Furthermore a revision is added in lines $235-237$ in section 4. \blockquote{The computational complexity of calculation of the predictive distribution is $O(N^3)$ as per section \label{sec:inference} this can limit the size of the covariance matrix constructed from the training data. Note that this computation overhead is paid for every unique assignment to the model hyper-parameters.}

\item \emph{Both the training and model selection data sets did not include strong geomagnetic storms (Dst was between -30 and +37 in the training set and in a similar range in the model selection set).} 

\textbf{Remarks}: Additions were made in lines $229-234$ \blockquote{Although the training and model selection data sets both do not have a geomagnetic storm in them, this would not degrade the performance of \emph{GP-AR} and \emph{GP-ARX} because the linear polynomial kernel describes a non stationary and self similar Gaussian Process. This implies that for two events where the time histories of $Dst$, $V$ and $B_z$ are not close to each other but can be expressed as a diagonal rescaling of time histories observed in the training data, the predictive distribution is a linearly rescaled version of the training data $Dst$ distribution.}

\item \emph{The time lag parameters 'p' for the two models were selected by comparing the model to the 63 storm events. Shouldn't the parameter have been selected using an independent set of storms instead in order not to favor these new models against the other models?}

\textbf{Remarks}: Additions were made in lines $242-247$ in section 4. \blockquote{The time lags chosen for \emph{GP-AR} are $p = 6$ while for \emph{GP-ARX} $p=6$, $p_{ex} = 1$ are arrived by experimenting with values starting from one and choosing the value of $p$ yielding best performance on the 63 events in the bench mark. We do not feel that this approach for choosing the time lags leads to bias towards models which only perform well on the data set of \cite{Ji2012} because Gaussian Processes are not easily susceptible to overfitting for situations when the number of model hyper-parameters is small or leave one out cross-validation is not performed.}

\item \emph{Rastaetter et al. please correct the first initial in "A. Glocer".} 

\textbf{Remarks}: Corrections made in lines $437-439$

\end{enumerate}

\section*{Acknowledgements}

The authors would like to thank the referee for their valuable feedback and critical assesment which helped to improve the quality of the article. 

\bibliography{swsc}


\end{document}